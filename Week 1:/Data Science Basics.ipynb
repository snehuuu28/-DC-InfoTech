{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b6e6e6-ef8e-416d-b595-c9779797360d",
   "metadata": {},
   "source": [
    "# Introduction to data science concepts, data types, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2743e2-44e8-434b-9159-defe31c83b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science Concepts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03a93b84-5df6-4de4-893a-8259eefab6e2",
   "metadata": {},
   "source": [
    "What is Data Science?  \n",
    "\n",
    "Data science is an interdisciplinary field that uses scientific methods, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It involves various processes such as data collection, data processing, analysis, visualization, and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9410811-04dc-4ae5-93f8-fc3268cea2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d9d4083-ae8f-4fcb-9c01-2c70ba8bb8bf",
   "metadata": {},
   "source": [
    "Data Types:\n",
    "\n",
    "1) Numerical Data:\n",
    "\n",
    " i) Continuous: Data that can take any value within a range (e.g., height, weight).\n",
    " ii)Discrete: Data that can only take certain values (e.g., number of students in a class).\n",
    "\n",
    "2) Categorical Data:\n",
    " \n",
    " i) Nominal: Data with categories that don't have a specific order (e.g., colors, gender).\n",
    " ii) Ordinal: Data with categories that have a specific order (e.g., education levels).\n",
    "\n",
    "3) Text Data: Free-form text such as sentences or words (e.g., customer reviews).\n",
    "\n",
    "4) Time Series Data: Data points collected or recorded at specific time intervals (e.g., stock prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71da3816-b44a-4044-ac32-cdba8026ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27bfe8df-04ba-47e4-a3e7-ef0e49f7f128",
   "metadata": {},
   "source": [
    "Basic Statistics:\n",
    "\n",
    "Mean: The average of a set of values.\n",
    "\n",
    "Median: The middle value in a set of values when arranged in order.\n",
    "\n",
    "Mode: The most frequently occurring value in a dataset.\n",
    "\n",
    "Standard Deviation: A measure of the amount of variation or dispersion in a set of values.\n",
    "\n",
    "Correlation: A measure of the relationship between two variables (ranges from -1 to 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abba55-23e0-42bc-a2d1-1eaff86ed80b",
   "metadata": {},
   "source": [
    "# Data preprocessing: handling missing values, normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c5cc71-0601-46af-8cb5-e2aa397267f3",
   "metadata": {},
   "source": [
    "# 1. Handling Missing Values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdd17bbf-67b9-404c-ab0d-385a3108dd20",
   "metadata": {},
   "source": [
    "1. Handling Missing Values\n",
    "\n",
    "Missing data is a common issue in datasets and can occur due to various reasons, such as data entry errors or unrecorded information. Handling missing values is crucial because they can skew the results and reduce the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95118b08-05ce-4eb2-bb94-20ac97de49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods to Handle Missing Values:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee5019c3-d86e-4c54-9cd5-a50b2ff2e838",
   "metadata": {},
   "source": [
    "Methods to Handle Missing Values:\n",
    " \n",
    "1)Removing Missing Data:\n",
    "\n",
    " i) Remove Rows: If the number of missing values is small, you can remove the rows with missing data.\n",
    "ii) Remove Columns: If an entire column has a large number of missing values, it might be better to remove it.\n",
    "\n",
    "2)Imputation:\n",
    "\n",
    " i) Mean/Median Imputation: Replace missing values with the mean or median of the column.\n",
    "ii) Mode Imputation: Replace missing values with the mode (most frequent value) of the column.\n",
    "iii)Forward/Backward Fill: Use the previous or next observation to fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511c8570-8730-467a-8e4c-24b526249486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b731ac-61f3-4d97-9317-8e129639730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Age   Salary\n",
      "0  25.000000  50000.0\n",
      "1  29.666667  60000.0\n",
      "2  29.000000  70000.0\n",
      "3  35.000000  80000.0\n",
      "4  29.666667  90000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {'Age': [25, np.nan, 29, 35, np.nan],\n",
    "        'Salary': [50000, 60000, np.nan, 80000, 90000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "df['Salary'].fillna(df['Salary'].median(), inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ceabb-a7c1-4e58-b7c6-75dcde9ecc40",
   "metadata": {},
   "source": [
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fffe75c4-fcc7-4d46-a88a-65e559c68a86",
   "metadata": {},
   "source": [
    "Normalization:\n",
    "\n",
    "Normalization is the process of scaling data to a specific range, usually [0, 1], to ensure that all features contribute equally to the analysis. This is important when features have different scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f028cc74-e838-4658-a3b4-c1f6fac410c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods of Normalization:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6427857b-7031-4855-b92d-cab64be5ddc1",
   "metadata": {},
   "source": [
    "Methods of Normalization:\n",
    "\n",
    "1) Min-Max Normalization:\n",
    "   Scales the data to a fixed range [0, 1].\n",
    "   Formula:\n",
    "         ùëãùëõùëúùëüùëö= (ùëã‚àíùëãùëöùëñùëõ)/(ùëãùëöùëéùë• ‚àí ùëãùëöùëñùëõ)\n",
    "‚Äã\n",
    "2) Z-Score Normalization (Standardization):\n",
    "     Scales the data based on the mean and standard deviation.\n",
    "     Formula: \n",
    "         ùëç = (ùëã‚àíùúá)/ùúé\n",
    "     Where,\n",
    "          Œº is the mean and \n",
    "          œÉ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9749e31d-61af-4292-8f6b-c07a2fa8ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2766cc3-eed2-4a24-bd93-d07bbcbb0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [100, 200, 300, 400, 500]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max Normalization\n",
    "scaler = MinMaxScaler()\n",
    "df[['Feature1', 'Feature2']] = scaler.fit_transform(df[['Feature1', 'Feature2']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c98a786-d19c-4c90-9554-bd7ad3030aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0 -1.414214 -1.414214\n",
      "1 -0.707107 -0.707107\n",
      "2  0.000000  0.000000\n",
      "3  0.707107  0.707107\n",
      "4  1.414214  1.414214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Feature1': [1, 2, 3, 4, 5],\n",
    "        'Feature2': [100, 200, 300, 400, 500]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Z-Score Normalization (Standardization)\n",
    "scaler = StandardScaler()\n",
    "df[['Feature1', 'Feature2']] = scaler.fit_transform(df[['Feature1', 'Feature2']])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fdc9a4-7a5f-48c7-ab9a-fc83d5fedc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
